<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Wells-fargo-analytics-competition by kadunn</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-dark.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="stylesheets/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>
      <div id="header">
        <nav>
          <li class="fork"><a href="https://github.com/kadunn/Wells-Fargo-Analytics-Competition">View On GitHub</a></li>
          <li class="downloads"><a href="https://github.com/kadunn/Wells-Fargo-Analytics-Competition/zipball/master">ZIP</a></li>
          <li class="downloads"><a href="https://github.com/kadunn/Wells-Fargo-Analytics-Competition/tarball/master">TAR</a></li>
          <li class="title">DOWNLOADS</li>
        </nav>
      </div><!-- end header -->

    <div class="wrapper">

      <section>
        <div id="title">
          <h1>Wells-fargo-analytics-competition</h1>
          <p></p>
          <hr>
          <span class="credits left">Project maintained by <a href="https://github.com/kadunn">kadunn</a></span>
          <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
        </div>

        <h1>
<a id="the-project" class="anchor" href="#the-project" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Project</h1>

<p>Social media represents a vast amount of data readily available for interpretation like never before. Analysis of social media has potential to wield monumental power. That being said, economy of scales comes into play-- the amount of data available is so vast that it is also daunting. It is up to those who may benefit from online social patterns to create something meaningful from a collection of seemingly independent online posts. It is the new age space race but instead of a competition between countries going to space, researchers scramble to develop the best algorithm for determining the drivers of online social media.</p>

<p>Data science enthusiasts were invited by Wells Fargo to analyze a data set containing posts from facebook and twitter about four banks. The data set was made up of about 100,000 posts from Facebook and Twitter. Posts were chosen for their mention of one of four banks. Bank names were changed to “BankA”, “BankB”, “BankC”, and “BankD” to preserve analytical integrity. </p>

<p>The objects of the challenge was to determine </p>

<p>What financial topics are discussed on social media and what is the cause of these conversations?
Are the topics and substance consistent across the industry or are they isolated to individual banks?</p>

<h2>
<a id="the-team" class="anchor" href="#the-team" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Team</h2>

<p>College of Charleston undergraduates in sections of the introductory level data course participated in the competition in teams. Analysis of the data set was done in R. The report that follows was the work of Samantha Quigley, Sonia Kopel, Courtney Proferra, and myself, Kendall Dunn. </p>

<h1>
<a id="the-report" class="anchor" href="#the-report" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Report</h1>

<h2>
<a id="cleaning-and-preparation" class="anchor" href="#cleaning-and-preparation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cleaning and Preparation</h2>

<p>Before sinking our teeth in, we prepared the raw data several functions from the text mining package in R. The text mining package is built for analyzing textual data frames. Using a corpus, it allows the programmer to organize the data, making cleaning and manipulation possible.</p>

<p>To clean the data, we applied the following:</p>

<p>Removed non ASCII’s</p>

<pre><code>df.texts.clean = as.data.frame(iconv(df.texts $FullText, "latin1", "ASCII", sub=""))
</code></pre>

<p>Removed Stopwords</p>

<p>for(j in seq(data)){
  corpus[[j]] &lt;- removeWords(corpus[[j]], stopwords("english"))
  corpus[[j]] &lt;- stemDocument(corpus[[j]])
}
Removed words that were not considered stop words yet interfered with analysis of the data frame</p>

<p>txt.corpus &lt;- tm_map(corpus, removeWords, c("banka","you","your", “was”, “for", “the", “and", “bankb", “bankc", “bankd",  ”bank”, ”name", “twithndl", “nameresp", “internet", "twithndlbanka", “twithndlbankc", “twithndlbankb”, "twithndl_bankd", “twit_hndl_banka",  “twit_hndl”, “twit_hndl_bankc”, “twit_hndl_bankb", “twithndl_bankd”,"bankds","bit",
 “twithndlbankd", “twithndlbankbhelp","https", "ift"))</p>

<p>Removed extra whitespace</p>

<p>txt.corpus &lt;- tm_map(txt.corpus, content_transformer(stripWhitespace))</p>

<h2>
<a id="what-financial-topics-are-discussed-on-social-media-and-what-is-the-cause-of-these-conversations" class="anchor" href="#what-financial-topics-are-discussed-on-social-media-and-what-is-the-cause-of-these-conversations" aria-hidden="true"><span class="octicon octicon-link"></span></a>What financial topics are discussed on social media and what is the cause of these conversations?</h2>

<h2>
<a id="frequent-terms" class="anchor" href="#frequent-terms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Frequent Terms</h2>

<p>To determine the content of the tweets, we identified frequent terms.</p>

<pre><code>freqterms = findFreqTerms(dtm.clean, 1014)
</code></pre>

<p>The above code returned the following:</p>

<p>The word cloud shows the weighted frequency of words in the data frame.</p>

<p>Now that we know what people are talking about, it is important to know what they are saying about it.</p>

<h2>
<a id="dendrogram" class="anchor" href="#dendrogram" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dendrogram</h2>

<p>We constructed a dendrogram to address this objective. The dendrogram was constructed using Euclidian distance formula. Words that are more strongly associated are grouped closer together on the dengrogram</p>

<p>dtm.clean &lt;- DocumentTermMatrix(txt.corpus)</p>

<p>d &lt;- dist(t(dtm.clean), method="euclidian")<br>
fit &lt;- hclust(d=d, method="ward.D")<br>
plot(fit)</p>

<p>If we look back to the list of frequent terms, it is clear that popular topics on Facebook and Twitter revolve largely around words that involve communication.     </p>

<p>share           tell            contact     discuss         phone   call            help            visit           see         service assistance      assist</p>

<p>While other trends in topic similarity do exist in the frequent terms list, those of communication were not only most prevalent, but are also most crucial. In order for any company to be successful, they must maintain a happy client base. Thus, the way the customer talks about topics that potentially concern contact with company personnel are of interest. The dendrogram makes it possible to decipher exactly this.</p>

<p>I have spliced the large dendrogram for ease of analysis so we can look at each communication term or subjects or related terms in depth. The spliced sections are presented in the order in which they appear on the dendrogram to preserve the distances of relationships sets have to one another.</p>

<p>In addition to presenting the dendrogram, I have included correlations of prominent words in the dendrogram to the use of other words to open the door to a deeper analysis.</p>

<h3>
<a id="service" class="anchor" href="#service" aria-hidden="true"><span class="octicon octicon-link"></span></a>Service</h3>

<p>Unsurprisingly, service is highly related to the use of the word customer, as seen in the phrase “customer service”. From this section of the dendrogram, and inference can be made that people often talk about getting information when using the word “service”.
Banks could use this information to sift through posts using these 12 words to determine exactly what information clients want from customer service at any given time by constructing a  new dendrogram made up of only tweets with any combination of these 12 words.</p>

<h3>
<a id="discuss" class="anchor" href="#discuss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discuss</h3>

<p>The proximity of “happy” and “discuss” is an indication that clients or employees are happy to have discussions with one another. 
Looking at the correlations, discuss, give, and happy are all included among the words most highly correlated with the word “please”.</p>

<blockquote>
<p>findAssocs(dtm.clean, "please", 0.1)
$please</p>
</blockquote>

<p>phone  assistance  help    call       discuss      like       happy      assist      best 
         0.27        0.26        0.24       0.19         0.16        0.16      0.15        0.13        0.13 </p>

<p>details     numbers    contact      give        info       information 
            0.12        0.12           0.11          0.11        0.11            0.10 </p>

<h3>
<a id="visit" class="anchor" href="#visit" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visit</h3>

<p>“Visit” on the dendrogram provides little evidence for a safe conclusion, however, using the correlations, there is a relationship between “call” and “visit”. The other words in the list for “call” are all positive, implying that people generally say positive things when talking about calls and possibly visits however this is only speculation and not a safe conclusion.</p>

<blockquote>
<p>findAssocs(dtm.clean, "call", 0.1)
$call
  phone     give  number  please   happy    best   visit 
   0.43    0.27    0.22    0.21    0.19    0.15    0.11    0.11 </p>
</blockquote>

<h3>
<a id="contact-and-called" class="anchor" href="#contact-and-called" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contact and Called</h3>

<p>The words “sorry” and “glad” are found to be close to the both “contact” and “called” in the dendrogram. The pairing of “sorry” with these words seems strange and is not found under any of the words that were a part of the correlation analysis from the frequent terms list.</p>

<h3>
<a id="share-and-appreciate" class="anchor" href="#share-and-appreciate" aria-hidden="true"><span class="octicon octicon-link"></span></a>Share and Appreciate</h3>

<p>“Share” and “appreciate” are near the words “information” and “thank”. This is an indication that clients are happy with the help they are getting and tweeting phrases such as “thank you for sharing ‘x’ information” in response. By isolating these words, a bank could gain insight as to how people are reacting to their attempts to communicate with the customer.</p>

<h3>
<a id="help-see-and-assistance" class="anchor" href="#help-see-and-assistance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Help, See, and Assistance</h3>

<p>Examples of possible phrases that may have cause these words to appear near each other in the dendrogram are:
“I need help”
“Can you help”
“I need assistance”
It is likely that when people use the words “assistance” and “help”, they are requesting assistance and help, rather than reviewing it. With this information, banks can pinpoint customer service questions by searching for posts containing these words. The words most closely correlated the help solidify this conclusion.</p>

<blockquote>
<p>findAssocs(dtm.clean, "help", 0.1)
$help
       see  following        can       need assistance        let     please   anything       know        how 
      0.40       0.36       0.33       0.31       0.30       0.28       0.24       0.22       0.22       0.21 
      like     dirmsg      happy        try      still       able      phone    account       info 
      0.18       0.17       0.17       0.17       0.16       0.15       0.11       0.10       0.10 </p>

<p>findAssocs(dtm.clean, "account", 0.1)
$account
numbers    tell     try     any   about    with  dirmsg  follow    help    like     not 
   0.32    0.18    0.17    0.13    0.12    0.12    0.11    0.11    0.10    0.10    0.10 </p>

<p>findAssocs(dtm.clean, "please", 0.1)
$please
     dirmsg         let       phone  assistance        help         can        need      follow         see 
       0.47        0.29        0.27        0.26        0.24        0.23        0.23        0.22        0.21 
       know        call   following    anything     discuss        like       happy      assist        best 
       0.20        0.19        0.19        0.17        0.16        0.16        0.15        0.13        0.13 
        any     details     numbers     contact        give        info       tweet a 
       0.12        0.12        0.12        0.11        0.11        0.11        0.11        0.10 </p>
</blockquote>

<h3>
<a id="assist" class="anchor" href="#assist" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assist</h3>

<p>“Assist” is correlated to the word “please”, which is also correlated  to words involving the need of assistance. To capture more posts that are requests for assistance, posts that use the word “assist” could also be added the the list.</p>

<blockquote>
<p>findAssocs(dtm.clean, "please", 0.1)
$please
     dirmsg         let       phone  assistance        help         can        need      follow         see 
       0.47        0.29        0.27        0.26        0.24        0.23        0.23        0.22        0.21 
       know        call   following    anything     discuss        like       happy      assist        best 
       0.20        0.19        0.19        0.17           0.16           0.16      0.15        0.13        0.13 
       any     details     numbers     contact        give        info       tweet a 
       0.12        0.12        0.12        0.11        0.11        0.11        0.11        0.10 </p>
</blockquote>

<p>While the determination of correlation and construction of a dendrogram are different methods and determine different things, using them in cohorts gives a fuller picture. The dendrogram shows what words are closely related, but following up with correlation helps determine what words are also connected to a word in a dendrogram but isn’t near by because of its closer proximity to another term. Essentially, using correlation sweeps up the missing pieces of the dendrogram. In this way, it is possible to develop a network of nodes and links between topics. By having the links pinpointed, a bank can screen posts for specific characteristics such as reviews or assistance and respond quickly and accurately without the need for a person that sifts through posts one by one-- saving time and money.</p>

<h2>
<a id="are-the-topics-and-substance-consistent-across-the-industry-or-are-they-isolated-to-individual-banks" class="anchor" href="#are-the-topics-and-substance-consistent-across-the-industry-or-are-they-isolated-to-individual-banks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Are the topics and substance consistent across the industry or are they isolated to individual banks?</h2>

<h3>
<a id="sentiment" class="anchor" href="#sentiment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sentiment</h3>

<p>The next natural step since the topics the posts concern has been addressed is the nature of the posts. Posts were separated by which bank they referenced and then a sentiment analysis was performed on the banks as individuals.</p>

<p>docs &lt;- Corpus(DataframeSource(dataset.txt))</p>

<p>bankA.idx = which(sapply(df$FullText,function(x) grepl("BankA",x)))
bankB.idx = which(sapply(df$FullText,function(x) grepl("BankB",x)))
bankC.idx = which(sapply(df$FullText,function(x) grepl("BankC",x)))
bankD.idx = which(sapply(df$FullText,function(x) grepl("BankD",x)))</p>

<p>df.A = df[bankA.idx,]
df.B = df[bankB.idx,]
df.C = df[bankC.idx,]
df.D = df[bankD.idx,]</p>

<p>pos &lt;- scan('positive-words.txt',what='character',comment.char=';')
neg &lt;- scan('negative-words.txt',what='character',comment.char=';')</p>

<h3>
<a id="t-tests" class="anchor" href="#t-tests" aria-hidden="true"><span class="octicon octicon-link"></span></a>T-Tests</h3>

<p>To compare the sentiment scores between banks, a t-test was performed. We felt confident in the validity of our t-test since the sentiment scores met the qualifications for being approximately normal, independent, and of equal variance. </p>

<p>Box plot for variance
The outliers are not concerning due to the size of the data frame.</p>

<p>Normal distribution
The violin plots show the normality of the data. The size of the data frame as well as the narrowness of the tails ensure the differences in the distributions are negligible. </p>

<p>The null hypothesis was that there would be no difference between the mean sentiments of a given bank compared to that of mean sentiment of the banks combined. Please see the attached document titled Statistical analysis to see each test. Looking at the violin plots, the normality assumption of the t-tests appears to be fulfilled as well as the assumption of equal variance. Bank c and d have shortened tails, however, due to the size of the data set, it can be determined that the t-test qualifies as an appropriate model. The outliers are of no concern due to the number of observations in the dataset and their affect is therefore negligible. 
Factors considered, it is safe to conclude a relationship exists between the bank being discussed in a tweet and the positivity or negativity of the language in the tweet. In every case, the p-value was overwhelmingly small (2.2 x 10-16). The null hypothesis can be rejected with 100% confidence. On average, the BankA posts scored higher on the sentiment analysis than the general posts. The mean sentiment of each of the remaining 3 banks was lower than the general average. BankD scored the worst, followed by BankC, and then BankB. </p>

<h2>
<a id="interpretation" class="anchor" href="#interpretation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Interpretation</h2>

<p>In the future, I would like to add in more variables to create a more dynamic and comprehensive analysis of a social media feed. By comparing the times of day and week to the sentiment, banks could determine when customers were happy and unhappy. After compiling this data, a weight factor based on the popularity of the post could also come into play, helping a media team decide when to act on tweets and which tweets to act on. Another future application is the comparison between sentiment and popularity of a post. If negative posts are more likely to be visible, then media teams should address them first. Again, a third factor could come into play. The visibility of certain topics and their sentiments could help determining precisely what is going poorly and well and what needs to be addressed due to the popularity of the topic. </p>

<p>The fundamentals of the potential future manipulations above are established by our report. The sentiment scores are in place, and we have begun to determine what topics are closely related. This is valuable because as the methods we used are put into practice, they may need to be manipulated as needs change so having a framework that is flexible is of the most importance.</p>
      </section>

    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
    
  </body>
</html>
