{"name":"Wells-fargo-analytics-competition","tagline":"","body":"#The Project\r\n\r\nSocial media represents a vast amount of data readily available for interpretation like never before. Analysis of social media has potential to wield monumental power. That being said, economy of scales comes into play-- the amount of data available is so vast that it is also daunting. It is up to those who may benefit from online social patterns to create something meaningful from a collection of seemingly independent online posts. It is the new age space race but instead of a competition between countries going to space, researchers scramble to develop the best algorithm for determining the drivers of online social media.\r\n\r\nData science enthusiasts were invited by Wells Fargo to analyze a data set containing posts from facebook and twitter about four banks. The data set was made up of about 100,000 posts from Facebook and Twitter. Posts were chosen for their mention of one of four banks. Bank names were changed to “BankA”, “BankB”, “BankC”, and “BankD” to preserve analytical integrity. \r\n\r\nThe objects of the challenge was to determine \r\n\r\nWhat financial topics are discussed on social media and what is the cause of these conversations?\r\nAre the topics and substance consistent across the industry or are they isolated to individual banks?\r\n\r\n##The Team\r\n\r\nCollege of Charleston undergraduates in sections of the introductory level data course participated in the competition in teams. Analysis of the data set was done in R. The report that follows was the work of Samantha Quigley, Sonia Kopel, Courtney Proferra, and myself, Kendall Dunn. \r\n\r\n#The Report\r\n\r\n##Cleaning and Preparation\r\n\r\nBefore sinking our teeth in, we prepared the raw data several functions from the text mining package in R. The text mining package is built for analyzing textual data frames. Using a corpus, it allows the programmer to organize the data, making cleaning and manipulation possible.\r\n\r\nTo clean the data, we applied the following:\r\n\t\r\nRemoved non ASCII’s\r\n\r\n\tdf.texts.clean = as.data.frame(iconv(df.texts $FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\n\t\r\nRemoved Stopwords\r\n\r\nfor(j in seq(data)){\r\n  corpus[[j]] <- removeWords(corpus[[j]], stopwords(\"english\"))\r\n  corpus[[j]] <- stemDocument(corpus[[j]])\r\n}\r\nRemoved words that were not considered stop words yet interfered with analysis of the data frame\r\n\r\ntxt.corpus <- tm_map(corpus, removeWords, c(\"banka\",\"you\",\"your\", “was”, “for\", “the\", “and\", “bankb\", “bankc\", “bankd\",  ”bank”, ”name\", “twithndl\", “nameresp\", “internet\", \"twithndlbanka\", “twithndlbankc\", “twithndlbankb”, \"twithndl_bankd\", “twit_hndl_banka\",  “twit_hndl”, “twit_hndl_bankc”, “twit_hndl_bankb\", “twithndl_bankd”,\"bankds\",\"bit\",\r\n “twithndlbankd\", “twithndlbankbhelp\",\"https\", \"ift\"))\r\n\r\nRemoved extra whitespace\r\n\r\ntxt.corpus <- tm_map(txt.corpus, content_transformer(stripWhitespace))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n##What financial topics are discussed on social media and what is the cause of these conversations?\r\n\t\r\n##Frequent Terms\r\nTo determine the content of the tweets, we identified frequent terms.\r\n\r\n\tfreqterms = findFreqTerms(dtm.clean, 1014)\r\n\r\nThe above code returned the following:\r\n\r\n\r\n\r\nThe word cloud shows the weighted frequency of words in the data frame.\r\n\r\nNow that we know what people are talking about, it is important to know what they are saying about it.\r\n\r\n##Dendrogram\r\nWe constructed a dendrogram to address this objective. The dendrogram was constructed using Euclidian distance formula. Words that are more strongly associated are grouped closer together on the dengrogram\r\n\r\ndtm.clean <- DocumentTermMatrix(txt.corpus)\r\n\r\nd <- dist(t(dtm.clean), method=\"euclidian\")   \r\nfit <- hclust(d=d, method=\"ward.D\")   \r\nplot(fit)\r\n\r\n\r\nIf we look back to the list of frequent terms, it is clear that popular topics on Facebook and Twitter revolve largely around words that involve communication. \t\r\n\r\nshare\t\t\ttell\t\t\tcontact\t\tdiscuss\t\t\tphone\tcall\t\t\thelp\t\t\tvisit\t\t\tsee\t\t\tservice\tassistance\t\tassist\r\n\r\nWhile other trends in topic similarity do exist in the frequent terms list, those of communication were not only most prevalent, but are also most crucial. In order for any company to be successful, they must maintain a happy client base. Thus, the way the customer talks about topics that potentially concern contact with company personnel are of interest. The dendrogram makes it possible to decipher exactly this.\r\n\r\nI have spliced the large dendrogram for ease of analysis so we can look at each communication term or subjects or related terms in depth. The spliced sections are presented in the order in which they appear on the dendrogram to preserve the distances of relationships sets have to one another.\r\n\r\nIn addition to presenting the dendrogram, I have included correlations of prominent words in the dendrogram to the use of other words to open the door to a deeper analysis.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n###Service\r\nUnsurprisingly, service is highly related to the use of the word customer, as seen in the phrase “customer service”. From this section of the dendrogram, and inference can be made that people often talk about getting information when using the word “service”.\r\nBanks could use this information to sift through posts using these 12 words to determine exactly what information clients want from customer service at any given time by constructing a  new dendrogram made up of only tweets with any combination of these 12 words.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n###Discuss\r\nThe proximity of “happy” and “discuss” is an indication that clients or employees are happy to have discussions with one another. \r\nLooking at the correlations, discuss, give, and happy are all included among the words most highly correlated with the word “please”.\r\n\r\n\r\n\r\n>findAssocs(dtm.clean, \"please\", 0.1)\r\n$please\r\n\r\n phone  assistance\thelp\tcall       discuss      like       happy      assist      best \r\n       \t 0.27        0.26        0.24       0.19         0.16        0.16      0.15        0.13        0.13 \r\n\r\ndetails     numbers    contact      give        info       information \r\n         \t0.12        0.12           0.11          0.11        0.11            0.10 \r\n\r\n###Visit\r\n“Visit” on the dendrogram provides little evidence for a safe conclusion, however, using the correlations, there is a relationship between “call” and “visit”. The other words in the list for “call” are all positive, implying that people generally say positive things when talking about calls and possibly visits however this is only speculation and not a safe conclusion.\r\n\r\n>findAssocs(dtm.clean, \"call\", 0.1)\r\n$call\r\n  phone     give  number  please   happy    best   visit \r\n   0.43    0.27    0.22    0.21    0.19    0.15    0.11    0.11 \r\n\r\n\r\n###Contact and Called\r\nThe words “sorry” and “glad” are found to be close to the both “contact” and “called” in the dendrogram. The pairing of “sorry” with these words seems strange and is not found under any of the words that were a part of the correlation analysis from the frequent terms list.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n###Share and Appreciate\r\n“Share” and “appreciate” are near the words “information” and “thank”. This is an indication that clients are happy with the help they are getting and tweeting phrases such as “thank you for sharing ‘x’ information” in response. By isolating these words, a bank could gain insight as to how people are reacting to their attempts to communicate with the customer.\r\n\r\n\r\n###Help, See, and Assistance\r\nExamples of possible phrases that may have cause these words to appear near each other in the dendrogram are:\r\n“I need help”\r\n“Can you help”\r\n“I need assistance”\r\nIt is likely that when people use the words “assistance” and “help”, they are requesting assistance and help, rather than reviewing it. With this information, banks can pinpoint customer service questions by searching for posts containing these words. The words most closely correlated the help solidify this conclusion.\r\n\r\n>findAssocs(dtm.clean, \"help\", 0.1)\r\n$help\r\n       see  following        can       need assistance        let     please   anything       know        how \r\n      0.40       0.36       0.33       0.31       0.30       0.28       0.24       0.22       0.22       0.21 \r\n      like     dirmsg      happy        try      still       able      phone    account       info \r\n      0.18       0.17       0.17       0.17       0.16       0.15       0.11       0.10       0.10 \r\n\r\n>findAssocs(dtm.clean, \"account\", 0.1)\r\n$account\r\nnumbers    tell     try     any   about    with  dirmsg  follow    help    like     not \r\n   0.32    0.18    0.17    0.13    0.12    0.12    0.11    0.11    0.10    0.10    0.10 \r\n\r\n>findAssocs(dtm.clean, \"please\", 0.1)\r\n$please\r\n     dirmsg         let       phone  assistance        help         can        need      follow         see \r\n       0.47        0.29        0.27        0.26        0.24        0.23        0.23        0.22        0.21 \r\n       know        call   following    anything     discuss        like       happy      assist        best \r\n       0.20        0.19        0.19        0.17        0.16        0.16        0.15        0.13        0.13 \r\n        any     details     numbers     contact        give        info       tweet a \r\n       0.12        0.12        0.12        0.11        0.11        0.11        0.11        0.10 \r\n\r\n###Assist\r\n“Assist” is correlated to the word “please”, which is also correlated  to words involving the need of assistance. To capture more posts that are requests for assistance, posts that use the word “assist” could also be added the the list.\r\n>findAssocs(dtm.clean, \"please\", 0.1)\r\n$please\r\n     dirmsg         let       phone  assistance        help         can        need      follow         see \r\n       0.47        0.29        0.27        0.26        0.24        0.23        0.23        0.22        0.21 \r\n       know        call   following    anything     discuss        like       happy      assist        best \r\n       0.20        0.19        0.19        0.17           0.16           0.16      0.15        0.13        0.13 \r\n       any     details     numbers     contact        give        info       tweet a \r\n       0.12        0.12        0.12        0.11        0.11        0.11        0.11        0.10 \r\n\r\n\r\n\r\n\r\n\r\nWhile the determination of correlation and construction of a dendrogram are different methods and determine different things, using them in cohorts gives a fuller picture. The dendrogram shows what words are closely related, but following up with correlation helps determine what words are also connected to a word in a dendrogram but isn’t near by because of its closer proximity to another term. Essentially, using correlation sweeps up the missing pieces of the dendrogram. In this way, it is possible to develop a network of nodes and links between topics. By having the links pinpointed, a bank can screen posts for specific characteristics such as reviews or assistance and respond quickly and accurately without the need for a person that sifts through posts one by one-- saving time and money.\r\n\r\n##Are the topics and substance consistent across the industry or are they isolated to individual banks?\r\n\r\n###Sentiment\r\nThe next natural step since the topics the posts concern has been addressed is the nature of the posts. Posts were separated by which bank they referenced and then a sentiment analysis was performed on the banks as individuals.\r\n\r\ndocs <- Corpus(DataframeSource(dataset.txt))\r\n\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\ndf.A = df[bankA.idx,]\r\ndf.B = df[bankB.idx,]\r\ndf.C = df[bankC.idx,]\r\ndf.D = df[bankD.idx,]\r\n\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\n###T-Tests\r\nTo compare the sentiment scores between banks, a t-test was performed. We felt confident in the validity of our t-test since the sentiment scores met the qualifications for being approximately normal, independent, and of equal variance. \r\n\r\nBox plot for variance\r\nThe outliers are not concerning due to the size of the data frame.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNormal distribution\r\nThe violin plots show the normality of the data. The size of the data frame as well as the narrowness of the tails ensure the differences in the distributions are negligible. \r\n\r\n\r\n\r\nThe null hypothesis was that there would be no difference between the mean sentiments of a given bank compared to that of mean sentiment of the banks combined. Please see the attached document titled Statistical analysis to see each test. Looking at the violin plots, the normality assumption of the t-tests appears to be fulfilled as well as the assumption of equal variance. Bank c and d have shortened tails, however, due to the size of the data set, it can be determined that the t-test qualifies as an appropriate model. The outliers are of no concern due to the number of observations in the dataset and their affect is therefore negligible. \r\nFactors considered, it is safe to conclude a relationship exists between the bank being discussed in a tweet and the positivity or negativity of the language in the tweet. In every case, the p-value was overwhelmingly small (2.2 x 10-16). The null hypothesis can be rejected with 100% confidence. On average, the BankA posts scored higher on the sentiment analysis than the general posts. The mean sentiment of each of the remaining 3 banks was lower than the general average. BankD scored the worst, followed by BankC, and then BankB. \r\n\r\n\r\n##Interpretation\r\n\r\nIn the future, I would like to add in more variables to create a more dynamic and comprehensive analysis of a social media feed. By comparing the times of day and week to the sentiment, banks could determine when customers were happy and unhappy. After compiling this data, a weight factor based on the popularity of the post could also come into play, helping a media team decide when to act on tweets and which tweets to act on. Another future application is the comparison between sentiment and popularity of a post. If negative posts are more likely to be visible, then media teams should address them first. Again, a third factor could come into play. The visibility of certain topics and their sentiments could help determining precisely what is going poorly and well and what needs to be addressed due to the popularity of the topic. \r\n\r\nThe fundamentals of the potential future manipulations above are established by our report. The sentiment scores are in place, and we have begun to determine what topics are closely related. This is valuable because as the methods we used are put into practice, they may need to be manipulated as needs change so having a framework that is flexible is of the most importance.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}